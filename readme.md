# Space-and-Speaker-Aware Acoustic Modeling with Effective Data Augmentation for Recognition of Multi-Array Conversational Speech

<p align="center">
  <a href="https://github.com/coalboss/SSA_AM/blob/main/LICENSE"><img alt="GitHub" src="https://img.shields.io/github/license/misitebao/standard-repository?style=flat-square"/></a>
  <a href="https://github.com/coalboss/SSA_AM"><img alt="GitHub Repo stars"src="https://img.shields.io/github/stars/coalboss/SSA_AM?style=flat-square"/></a>
  <a href="https://github.com/coalboss"><img alt="GitHub Repo stars" src="https://img.shields.io/badge/author-coalboss-brightgreen?style=flat-square"/></a>
</p>

<span id="nav-1"></span>

<div align="center"><img src="img/overall.jpg" width="640"/></div>

This repository provides a official implementation of our champion system of Track 1 of the CHiME-6 Challenge. 

## Citation

If you find this code useful in your research, please consider to cite the following papers:

```bibtex
@inproceedings{chai2023ssa,
  title={Li Chai and Hang Chen and Jun Du and Qing-Feng Liu and Chin-Hui Lee},
  booktitle={Space-and-Speaker-Aware Acoustic Modeling with Effective Data Augmentation for Recognition of Multi-Array Conversational Speech},
  pages={},
  year={},
  organization={}
}
```

## Introduction

  Our system is largely based on [the baseline system in Kaldi](https://github.com/kaldi-asr/kaldi/tree/master/egs/chime6/s5b_track1), with updated training data and acoustic model. To use our system, researchers must first prepare the data (including i-vector) and train the GMM-HMM model using the baseline system, following the instructions provided in the Kaldi repository. Once this is done, they can copy our code to the baseline folder and run it using the instructions provided in our repository.

## Training Data Preparation

  - **GSS and worn microphone**
  same as in baseline

  - ****

## Acoustic Model

## Results



### Please star it, thank you! :ï¼‰
